import torch
from torch_geometric.data import Data
from torch_geometric.utils import (to_undirected, 
                                   structured_negative_sampling, 
                                   negative_sampling, 
                                   dropout_edge, dropout_node, subgraph)
import logging
from torch_geometric.data.separate import separate
import copy
import os
import json
import datetime

def get_pos_neg_edges(
        g, sample_type='structured', 
        force_undirected=False, 
        neg_ratio=1.0,
    ):
    r""" we got 3 types target edges so far, cc_p2n, cc_p2p, cc_n2n.
    So we need to generate negative edges for each target edge type.
    This is a balanced data version.
    Args:
        g (pyg graph): the orignal homogenous graph.
        type (string): 'global' sampling or 'structured' sampling.
        force_undirected (bool): whether negative edges are undirected
        neg_ratio (float): average num of negative edges per positive edge
        sample_ratio (float): sample ratio for all edges
    Return:
        pos_edge_index (LongTensor 2xN), neg_edge_index (LongTensor 2xN),
        neg_edge_type (LongTensor N)
    """
    if neg_ratio > 1.0 or sample_type == 'global':
        neg_edge_index = negative_sampling(
            g.tar_edge_index, g.num_nodes,
            force_undirected=force_undirected,
            num_neg_samples=int(g.tar_edge_index.size(1)*neg_ratio),
        )
        
        neg_edge_type = torch.zeros(neg_edge_index.size(1), dtype=torch.long)
        for i in range(neg_edge_index.size(1)):
            node_pair = neg_edge_index[:, i]
            ntypes = set(g.node_type[node_pair].tolist())
            # for neg edge types are related to target edge types
            # weight for neg Cc_p2n
            if ntypes == {0, 2}: 
                neg_edge_type[i] = 2
            # weight for Cc_p2p
            elif ntypes == {2}:
                neg_edge_type[i] = 3
            # weight for Cc_n2n
            elif ntypes == {0}:
                neg_edge_type[i] = 4
        
        legal_mask = neg_edge_type > 0
        print(
            f"Using global negtive sampling, #pos={g.tar_edge_index.size(1)}, " + 
            f"#neg={neg_edge_index[:, legal_mask].size(1)}")
        return g.tar_edge_index, neg_edge_index[:,legal_mask], neg_edge_type[legal_mask]
    
    neg_edge_index = []
    neg_edge_type  = []
    tar_edge_offset = 0
    for i in range(g.tar_edge_dist.size(0)):
        pos_edges = g.tar_edge_index[:, tar_edge_offset:g.tar_edge_dist[i]+tar_edge_offset]
        pos_edge_src, pos_edge_dst, neg_edge_dst = structured_negative_sampling(
            pos_edges, g.num_nodes, contains_neg_self_loops=False,
        )
        tar_edge_offset += g.tar_edge_dist[i]
        # neg edge sampling
        indices = torch.randperm(neg_edge_dst.size(0))[
            :int(neg_edge_dst.size(0) * neg_ratio), 
        ]
        neg_edge_index.append(
            torch.stack((pos_edge_src[indices], neg_edge_dst[indices]), dim=0)
        )
        neg_edge_type += [i + g.tar_edge_type[0]] * indices.size(0)

        print(
            f"Using structured negtive sampling for target etype {i}, " + 
            f"pos={pos_edges.size(1)}, #neg={neg_edge_index[-1].size(1)}")
    
    return torch.cat(neg_edge_index, 1), torch.tensor(neg_edge_type)

def get_balanced_edges(
    g, neg_edge_index, neg_edge_type,
    neg_edge_ratio, sample_ratio = 1.0,
):
    r""" Get balanced edges according to their etypes, including both pos & neg edges.
    
    Args:
        g (Data): provides positive target edges and edge types.
        neg_edge_index (Tensor[2 x N]): neg edges generated by get_pos_neg_edges().
        neg_edge_type (Tensor[N]): neg edge types corresponding to pos edges.
        neg_edge_ratio (float): #neg edges per pos edge.
        sample_ratio (float): the proportion of samples to train.
    
    Return:
        pos_edge_index (Tensor [2, Npos]).
        pos_edge_type (Tensor [Npos]).
        pos_edge_y (Tensor [Npos]): target Cc values.
        neg_edge_index (Tensor [2, Nneg]).
        neg_edge_type (Tensor [Nneg]).
    """
    tar_edge_offset = 0
    min_edge_num = g.tar_edge_dist.min()
    neg_edge_index_list = []
    neg_edge_type_list  = []
    pos_edge_index_list = []
    pos_edge_type_list  = []
    pos_edge_y_list  = []
    for i in range(g.tar_edge_dist.size(0)):
        # for pos edges
        pos_edges = g.tar_edge_index[
            :, tar_edge_offset:g.tar_edge_dist[i]+tar_edge_offset
        ]
        pos_etypes = g.tar_edge_type[
            tar_edge_offset:g.tar_edge_dist[i]+tar_edge_offset
        ]
        pos_edge_y = g.tar_edge_y[
            tar_edge_offset:g.tar_edge_dist[i]+tar_edge_offset
        ]
        tar_edge_offset += g.tar_edge_dist[i]

        indices = torch.randperm(pos_edges.size(1))[
            :int(min_edge_num * sample_ratio), 
        ]
        pos_edge_index_list.append(pos_edges[:, indices])
        pos_edge_type_list.append(pos_etypes[indices])
        pos_edge_y_list.append(pos_edge_y[indices])
        
        logging.info(f"Edge type {i}, balanced pos edge num: {pos_edge_type_list[-1].size(0)}")

        # for neg edges
        neg_edge_mask = (neg_edge_type - neg_edge_type.min()) == i
        assert neg_edge_index.size(1) == neg_edge_type.size(0)
        neg_edges = neg_edge_index[:, neg_edge_mask]
        neg_etypes = neg_edge_type[neg_edge_mask]
        indices = torch.randperm(neg_edges.size(1))[
            :int(min_edge_num * neg_edge_ratio * sample_ratio), 
        ]
        neg_edge_index_list.append(neg_edges[:, indices])
        neg_edge_type_list.append(neg_etypes[indices])

        logging.info(f"Edge type {i}, balanced neg edge num: {neg_edge_type_list[-1].size(0)}")

    return (torch.cat(pos_edge_index_list, 1), torch.cat(pos_edge_type_list), 
            torch.cat(pos_edge_y_list),  # these are target Cc values
            torch.cat(neg_edge_index_list, 1), torch.cat(neg_edge_type_list),)



def collated_data_separate(data: Data, slices, idx: int=None):
    r""" 
    Reverse to InMemoryDataset.collate(),
    return the separated data_list from whole data chunk.
    """
    if idx is not None:
        separated_data = separate(
            cls=data.__class__,
            batch=data,
            idx=idx,
            slice_dict=slices,
            decrement=False,
        )
        return copy.copy(separated_data)
    
    data_list = []
    for i in range(data.y.size(0)):
        separated_data = separate(
            cls=data.__class__,
            batch=data,
            idx=i,
            slice_dict=slices,
            decrement=False,
        )
        data_list.append(copy.copy(separated_data))
    return data_list

# 修改 utils.py 中的 save_cap_model 函数

def save_cap_model(model, model_name, dataset_name, epoch, params=None, is_best=False, metrics=None):
    """
    保存模型权重和训练信息
    
    参数:
        model: 要保存的模型
        model_name (str): 模型名称 ('CapClassifier' 或 'CapRegressor')
        dataset_name (str): 数据集名称
        epoch (int): 当前训练轮数
        params (dict): 模型参数字典，用于标识模型
        is_best (bool): 是否是目前最优模型
        metrics (dict): 训练指标，如损失和准确率
    """
    checkpoint_dir = os.path.join(f"./models_dlpl-cap-{model_name.lower()}", dataset_name)
    os.makedirs(checkpoint_dir, exist_ok=True)
    
    # 构建基础文件名
    if params:
        # 只关注指定的几个关键参数
        important_params = {
            'num_hops': 'nh',
            'task': 'tsk',
            'num_gnn_layers': 'ngl',
            'num_head_layers': 'nhl',
            'hid_dim': 'hd',
            'use_stats': 'us',
            'use_bn': 'bn',
            'dropout': 'dp'
        }
        
        param_str = ""
        for full_name, short_name in important_params.items():
            if full_name in params:
                value = params[full_name]
                param_str += f"{short_name}_{value}_"
        
        param_str = param_str.rstrip("_")
        
        base_filename = f"dlpl-cap-{model_name.lower()}_{param_str}"
    else:
        base_filename = f"dlpl-cap-{model_name.lower()}"
    
    # 确保文件名总长度不超过限制（通常为255个字符）
    max_filename_length = 200  # 留一些余量给目录路径
    if len(f"{base_filename}_epoch_{epoch}.pth") > max_filename_length:
        # 截断基础文件名
        available_length = max_filename_length - len(f"_epoch_{epoch}.pth")
        base_filename = base_filename[:available_length]
    
    # 仅在关键epoch或最后一个epoch保存带时间戳的模型，减少I/O操作
    save_milestone = (epoch % 10 == 0) or (epoch == params.get('epochs', 0) - 1) if params else True
    
    if save_milestone:
        timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
        model_path = os.path.join(checkpoint_dir, f"{base_filename}_epoch_{epoch}_{timestamp}.pth")
        torch.save(model.state_dict(), model_path)
    
    # 始终保存作为最新模型
    latest_path = os.path.join(checkpoint_dir, f"{base_filename}_latest.pth")
    torch.save(model.state_dict(), latest_path)
    
    # 如果是最佳模型，另存一份
    if is_best:
        best_path = os.path.join(checkpoint_dir, f"{base_filename}_best.pth")
        torch.save(model.state_dict(), best_path)
    
    # 保存训练信息，但仅在保存模型的时候
    if metrics and save_milestone:
        info = {
            "model_name": model_name,
            "dataset_name": dataset_name,
            "epoch": epoch,
            "timestamp": datetime.datetime.now().strftime("%Y%m%d_%H%M%S"),
            "params": params,
            "metrics": metrics,
            "is_best": is_best
        }
        info_path = os.path.join(checkpoint_dir, f"{base_filename}_info.json")
        with open(info_path, "w") as f:
            json.dump(info, f, indent=4)
    
    if save_milestone:
        return model_path
    else:
        return latest_path

def load_cap_model(model, model_name, dataset_name, params=None, load_best=False):
    """
    加载已保存的模型权重
    
    参数:
        model: 要加载权重的模型
        model_name (str): 模型名称 ('CapClassifier' 或 'CapRegressor')
        dataset_name (str): 数据集名称
        params (dict): 模型参数字典，用于标识模型
        load_best (bool): 是否加载最佳模型，False则加载最新模型
    
    返回:
        bool: 是否成功加载模型
    """
    checkpoint_dir = os.path.join(f"./models_dlpl-cap-{model_name.lower()}", dataset_name)
    
    if not os.path.exists(checkpoint_dir):
        print(f"模型目录不存在: {checkpoint_dir}")
        return False
    
    if params:
        # 只关注指定的几个关键参数
        important_params = {
            'num_hops': 'nh',
            'task': 'tsk',
            'num_gnn_layers': 'ngl',
            'num_head_layers': 'nhl',
            'hid_dim': 'hd',
            'use_stats': 'us',
            'use_bn': 'bn',
            'dropout': 'dp'
        }
        
        param_str = ""
        for full_name, short_name in important_params.items():
            if full_name in params:
                value = params[full_name]
                param_str += f"{short_name}_{value}_"
        
        param_str = param_str.rstrip("_")
        
        base_filename = f"dlpl-cap-{model_name.lower()}_{param_str}"
    else:
        base_filename = f"dlpl-cap-{model_name.lower()}"
    
    # 确定要加载的模型路径
    if load_best:
        model_path = os.path.join(checkpoint_dir, f"{base_filename}_best.pth")
        if not os.path.exists(model_path):
            print(f"最佳模型不存在: {model_path}")
            return False
    else:
        model_path = os.path.join(checkpoint_dir, f"{base_filename}_latest.pth")
        if not os.path.exists(model_path):
            # 尝试寻找最新的模型文件
            model_files = [f for f in os.listdir(checkpoint_dir) 
                          if f.startswith(base_filename) and f.endswith(".pth")]
            if not model_files:
                print(f"未找到匹配的模型文件: {base_filename}*.pth")
                return False
            
            # 按照修改时间排序，找出最新的模型
            model_files.sort(key=lambda x: os.path.getmtime(os.path.join(checkpoint_dir, x)), reverse=True)
            model_path = os.path.join(checkpoint_dir, model_files[0])
    
    try:
        model.load_state_dict(torch.load(model_path, map_location=torch.device('cuda' if torch.cuda.is_available() else 'cpu')))
        print(f"成功加载模型: {model_path}")
        return True
    except Exception as e:
        print(f"加载模型失败: {e}")
        return False

def check_cap_model_exists(model_name, dataset_name, params=None):
    """
    检查指定参数的模型是否已经存在
    
    参数:
        model_name (str): 模型名称 ('CapClassifier' 或 'CapRegressor')
        dataset_name (str): 数据集名称
        params (dict): 模型参数字典，用于标识模型
    
    返回:
        tuple: (模型存在标志, 模型路径), 如果模型不存在，模型路径为应保存的路径
    """
    # 确保模型保存目录存在
    checkpoint_dir = os.path.join(f"./models_dlpl-cap-{model_name.lower()}", dataset_name)
    os.makedirs(checkpoint_dir, exist_ok=True)
    
    # 如果没有参数，尝试加载最新的模型
    if params is None:
        model_files = [f for f in os.listdir(checkpoint_dir) if f.endswith(".pth")]
        if not model_files:
            return False, os.path.join(checkpoint_dir, f"dlpl-cap-{model_name.lower()}_latest.pth")
        
        # 按照修改时间排序，找出最新的模型
        model_files.sort(key=lambda x: os.path.getmtime(os.path.join(checkpoint_dir, x)), reverse=True)
        return True, os.path.join(checkpoint_dir, model_files[0])
    
    # 只关注指定的几个关键参数
    important_params = {
        'num_hops': 'nh',
        'task': 'tsk',
        'num_gnn_layers': 'ngl',
        'num_head_layers': 'nhl',
        'hid_dim': 'hd',
        'use_stats': 'us',
        'use_bn': 'bn',
        'dropout': 'dp'
    }
    
    param_str = ""
    for full_name, short_name in important_params.items():
        if full_name in params:
            value = params[full_name]
            param_str += f"{short_name}_{value}_"
    
    param_str = param_str.rstrip("_")
    
    model_path = os.path.join(checkpoint_dir, f"dlpl-cap-{model_name.lower()}_{param_str}_latest.pth")
    
    return os.path.exists(model_path), model_path

def get_model_params_dict(args, exclude_keys=None):
    """
    从args中提取模型相关参数，创建一个字典用于标识模型
    
    参数:
        args: 参数对象
        exclude_keys (list): 要排除的参数名列表
    
    返回:
        dict: 模型参数字典
    """
    if exclude_keys is None:
        exclude_keys = ['num_workers', 'gpu', 'epochs', 'batch_size', 'seed']
    
    params = {}
    for key, value in vars(args).items():
        if key not in exclude_keys and not key.startswith('_'):
            params[key] = value
    
    return params